# 2026-01-02

## Focus
Decision Tree and Naive Bayes classifiers.

## Learning
- Decision Tree splits data using impurity measures. It uses entropy, information gain and gini index to calculate impurity
- entropy value lies between 0-1 and gini lies between 0-0.5
- gini impurity is more efficient than entropy in terms of computation power
- Risk of overfitting in deep trees, so we should mention maximum depth of the tree
- Naive Bayes uses Bayes theorem with independence assumption
- Difference between Gaussian and Multinomial Naive Bayes:
- for continuous data we use Gaussian Naive Bayes
- for texual data, where may be certain words are repeated, we supposed to classify the document based on such frequencies of words, then we use Multinomial Naive Bayes
- We use CountVectorization to convert text into pieces called 'tokens'
-

## Action
Studied core theory and mathematical implementation.

## Friction
Need hands-on practice to understand trade-offs.

## Next
Implement small classification examples.
